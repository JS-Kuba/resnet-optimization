{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tUCJ0kT1J602"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO & tips:\n",
        "# - hp tuning - wg instrukcji\n",
        "# - zrobić kwantyzacje post training\n",
        "# - pomiary czasu inferencji (testu) na baseline, pruned i quantized\n",
        "# - pomiary wielkości modelu (pliku/wymaganej pamięci)\n",
        "# - ustawiać nazwę swojego GPU (przy każdym wandb init)\n",
        "# - można wyłączać logowanie\n",
        "# - prezentacja, dokumentacja\n",
        "# - uwaga bo pliki modeli się nadpisują"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETAP DODATKOWY - Inicjalizacja logowania"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPU_1 = \"NVIDIA GeForce GTX 1060 6GB\"\n",
        "GPU_2 = \"NVIDIA GeForce RTX 3060 12GB\"\n",
        "\n",
        "PICKED_GPU = GPU_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WandbLogger:\n",
        "    def __init__(self, disable_logging) -> None:\n",
        "        self.disable_logging = disable_logging\n",
        "\n",
        "    def initialize(self, config, name, gpu):\n",
        "        if not self.disable_logging:\n",
        "            config[\"gpu\"] = gpu\n",
        "\n",
        "            wandb.init(\n",
        "                project=\"ososn-project\",\n",
        "                name=name,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "    def log_loss(self, train_loss, val_loss, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=step)\n",
        "\n",
        "    def log_sparsity(self, sparsity, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({\"sparsity\": sparsity}, step=step)\n",
        "\n",
        "    def log_time(self, time):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({\"train_time\": round(time, 3)})\n",
        "\n",
        "    def log_accuracy(self, accuracy, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} accuracy\": accuracy}, step=step)\n",
        "\n",
        "    def log_f1_score(self, f1_score, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} f1_score\": f1_score}, step=step)\n",
        "\n",
        "    def log_recall(self, recall, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} recall\": recall}, step=step)\n",
        "\n",
        "    def log_precision(self, precision, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} precision\": precision}, step=step)\n",
        "    \n",
        "    def finish(self):\n",
        "        if not self.disable_logging:\n",
        "            wandb.finish()\n",
        "\n",
        "class Report:\n",
        "    @staticmethod\n",
        "    def report_results(y_true, y_pred, tag, wandb_logger, step=None):\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        wandb_logger.log_accuracy(acc, tag, step)\n",
        "\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        if tag == \"test\":\n",
        "            wandb_logger.log_f1_score(f1, tag, step)\n",
        "            wandb_logger.log_recall(recall, tag, step)\n",
        "            wandb_logger.log_precision(precision, tag, step)\n",
        "        \n",
        "        return acc, f1, recall, precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etap 2 - Wybór danych i modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11,227,812 (11.23M)\n",
            "\n",
            "Epoch 1/1\n",
            "Train Loss: 2.0291, Acc: 0.4532\n",
            "Epoch training time: 143.89 sec\n",
            "Peak memory usage (training): 2022.98 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Loss: 2.0424, Acc: 0.4624\n",
            "Training finished.\n",
            "\n",
            "Test Acc: 0.4610, F1: 0.4526, Recall: 0.4610, Precision: 0.5835\n",
            "Inference Time (total): 16.62 sec\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "hp_sanity = {\n",
        "    \"num_epochs\": 1,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 64,\n",
        "    \"model_name\": \"resnet18\",\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# === TRANSFORMACJE ===\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "if not os.path.exists('./data/cifar-100-python'):\n",
        "    download = True\n",
        "else:\n",
        "    download = False\n",
        "\n",
        "train_set_full = torchvision.datasets.CIFAR100(root='./data', train=True, download=download, transform=transform_train)\n",
        "test_set_full = torchvision.datasets.CIFAR100(root='./data', train=False, download=download, transform=transform_test)\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(0.9 * len(train_set_full))\n",
        "val_len = len(train_set_full) - train_len\n",
        "train_subset, val_subset = random_split(train_set_full, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "train_subset.dataset.transform = transform_train\n",
        "val_subset.dataset.transform = transform_test\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=hp_sanity[\"batch_size\"], shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=hp_sanity[\"batch_size\"], shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set_full, batch_size=hp_sanity[\"batch_size\"], shuffle=False)\n",
        "\n",
        "wandb_logger = WandbLogger(disable_logging=True)\n",
        "wandb_logger.initialize(\n",
        "    config=hp_sanity,\n",
        "    name=f\"{hp_sanity['model_name']}-baseline-bs{hp_sanity['batch_size']}\",\n",
        "    gpu=PICKED_GPU,\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "\n",
        "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model: {hp_sanity['model_name']}, Parameters: {model_size:,} ({model_size / 1e6:.2f}M)\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hp_sanity[\"learning_rate\"])\n",
        "\n",
        "    def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_acc = 0.0\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            start_epoch = time.time()\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_time = time.time() - start_epoch\n",
        "            epoch_train_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_train_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            mem_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
        "\n",
        "            print(f\"Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}\")\n",
        "            print(f\"Epoch training time: {epoch_time:.2f} sec\")\n",
        "            print(f\"Peak memory usage (training): {mem_allocated:.2f} MB\")\n",
        "\n",
        "            wandb_logger.log_accuracy(epoch_train_acc, \"train\", epoch + 1)\n",
        "            wandb_logger.log_loss(epoch_train_loss, 0.0, epoch + 1)  # dummy val loss here\n",
        "            wandb_logger.log_time(epoch_time)\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "            val_acc, val_f1, val_recall, val_precision = Report.report_results(val_labels, val_preds, tag=\"val\", wandb_logger=wandb_logger, step=epoch + 1)\n",
        "\n",
        "            print(f\"Val Loss: {epoch_val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "            wandb_logger.log_loss(epoch_train_loss, epoch_val_loss, epoch + 1)\n",
        "\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(\"Training finished.\")\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        return model\n",
        "\n",
        "    def evaluate_model(model, dataloader, wandb_logger):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        test_acc, test_f1, test_recall, test_precision = Report.report_results(all_labels, all_preds, tag=\"test\", wandb_logger=wandb_logger)\n",
        "        print(f\"\\nTest Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Recall: {test_recall:.4f}, Precision: {test_precision:.4f}\")\n",
        "        print(f\"Inference Time (total): {inference_time:.2f} sec\")\n",
        "\n",
        "    trained_model = train_model(model, train_loader, optimizer, criterion, hp_sanity[\"num_epochs\"])\n",
        "    evaluate_model(trained_model, test_loader, wandb_logger)\n",
        "\n",
        "finally:\n",
        "    wandb_logger.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETAP TRZECI - Określenie zadania docelowego i przygotowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "hp_baseline = {\n",
        "        \"num_epochs\": 5,\n",
        "        \"learning_rate\": 1e-3,\n",
        "        # \"weight_decay\": 1e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"model_name\": \"resnet18\",\n",
        "        \"noise_level\": 0.05\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "B8uD0a9eiC39"
      },
      "outputs": [],
      "source": [
        "# === MAPOWANIE KLAS ===\n",
        "class_mapping = {\n",
        "    0: [4, 30, 55, 72, 95],\n",
        "    1: [1, 32, 67, 73, 91],\n",
        "    2: [54, 62, 70, 82, 92],\n",
        "    3: [9, 10, 16, 28, 61],\n",
        "    4: [0, 51, 53, 57, 83],\n",
        "    5: [22, 39, 40, 86, 87],\n",
        "    6: [5, 20, 25, 84, 94],\n",
        "    7: [6, 7, 14, 18, 24],\n",
        "    8: [3, 42, 43, 88, 97],\n",
        "    9: [12, 17, 37, 68, 76],\n",
        "    10: [23, 33, 49, 60, 71],\n",
        "    11: [15, 19, 21, 31, 38],\n",
        "    12: [34, 63, 64, 66, 75],\n",
        "    13: [26, 45, 77, 79, 99],\n",
        "    14: [2, 11, 35, 46, 98],\n",
        "    15: [27, 29, 44, 78, 93],\n",
        "    16: [36, 50, 65, 74, 80],\n",
        "    17: [47, 52, 56, 59, 96],\n",
        "    18: [8, 13, 48, 58, 90],\n",
        "    19: [41, 69, 81, 85, 89]\n",
        "}\n",
        "\n",
        "custom_class_names = [\n",
        "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables',\n",
        "    'household electrical device', 'household furniture', 'insects', 'large carnivores',\n",
        "    'large man-made outdoor things', 'large natural outdoor scenes', 'large omnivores and herbivores',\n",
        "    'medium-sized mammals', 'non-insect invertebrates', 'people', 'reptiles',\n",
        "    'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
        "]\n",
        "\n",
        "label_remap = {orig: new for new, orig_list in class_mapping.items() for orig in orig_list}\n",
        "NUM_CLASSES = len(class_mapping)\n",
        "\n",
        "# === PRZYGOTOWANIE DANYCH ===\n",
        "if not os.path.exists('./data/cifar-100-python'):\n",
        "    download = True\n",
        "else:\n",
        "    download = False\n",
        "\n",
        "train_set_full = torchvision.datasets.CIFAR100(root='./data', train=True, download=download, transform=transform_train)\n",
        "test_set_full = torchvision.datasets.CIFAR100(root='./data', train=False, download=download, transform=transform_test)\n",
        "\n",
        "\n",
        "# === TRANSFORMACJE ===\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                         std=[0.2673, 0.2564, 0.2761]),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409],\n",
        "                         std=[0.2673, 0.2564, 0.2761]),\n",
        "])\n",
        "\n",
        "\n",
        "def remap_dataset(dataset):\n",
        "    images, labels = [], []\n",
        "    for img, label in zip(dataset.data, dataset.targets):\n",
        "        if label in label_remap:\n",
        "            images.append(img)\n",
        "            labels.append(label_remap[label])\n",
        "    dataset.data = images\n",
        "    dataset.targets = labels\n",
        "    dataset.classes = custom_class_names\n",
        "    return dataset\n",
        "\n",
        "train_set = remap_dataset(train_set_full)\n",
        "test_set = remap_dataset(test_set_full)\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(0.9 * len(train_set))\n",
        "val_len = len(train_set) - train_len\n",
        "train_subset, val_subset = random_split(train_set, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "# Szumienie etykiet (10%)\n",
        "def add_label_noise(dataset, noise_level=0.1):\n",
        "    if isinstance(dataset, torch.utils.data.Subset):\n",
        "        for i in range(len(dataset)):\n",
        "            if random.random() < noise_level:\n",
        "                true_index = dataset.indices[i]\n",
        "                dataset.dataset.targets[true_index] = random.randint(0, NUM_CLASSES - 1)\n",
        "    else:\n",
        "        for i in range(len(dataset.targets)):\n",
        "            if random.random() < noise_level:\n",
        "                dataset.targets[i] = random.randint(0, NUM_CLASSES - 1)\n",
        "    return dataset\n",
        "\n",
        "train_subset.dataset.transform = transform_train\n",
        "val_subset.dataset.transform = transform_test\n",
        "\n",
        "train_set = add_label_noise(train_set, noise_level=hp_baseline[\"noise_level\"])\n",
        "\n",
        "# Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=hp_baseline[\"batch_size\"], shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=hp_baseline[\"batch_size\"], shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=hp_baseline[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETAP 4 - Douczanie modelu odniesienia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\tomek\\Desktop\\projects\\OSiOwSN_projekt\\resnet-optimization\\wandb\\run-20250612_134459-lp58tvjj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zpd-project/ososn-project/runs/lp58tvjj' target=\"_blank\">resnet18-baseline-bs64-noise0.05</a></strong> to <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zpd-project/ososn-project/runs/lp58tvjj' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/lp58tvjj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11.19M\n",
            "Epoch 1/5\n",
            "Train loss: 1.5134, Train acc: 0.5583, Val loss: 1.2706, Val acc 0.6374\n",
            "Epoch 2/5\n",
            "Train loss: 1.0947, Train acc: 0.6953, Val loss: 1.2433, Val acc 0.6496\n",
            "Epoch 3/5\n",
            "Train loss: 0.8805, Train acc: 0.7612, Val loss: 1.1015, Val acc 0.7062\n",
            "Epoch 4/5\n",
            "Train loss: 0.6731, Train acc: 0.8203, Val loss: 1.1597, Val acc 0.7020\n",
            "Epoch 5/5\n",
            "Train loss: 0.4844, Train acc: 0.8685, Val loss: 1.3400, Val acc 0.6770\n",
            "Training complete in 13m 46s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 0.7216, Test f1: 0.7202, Test recall: 0.7216, Test precision 0.7493\n",
            "Inference time: 18.53s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test accuracy</td><td>▁</td></tr><tr><td>test f1_score</td><td>▁</td></tr><tr><td>test precision</td><td>▁</td></tr><tr><td>test recall</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁▄▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▂▁</td></tr><tr><td>train_time</td><td>▁</td></tr><tr><td>val accuracy</td><td>▁▂██▅</td></tr><tr><td>val_loss</td><td>▆▅▁▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test accuracy</td><td>0.7216</td></tr><tr><td>test f1_score</td><td>0.72023</td></tr><tr><td>test precision</td><td>0.74934</td></tr><tr><td>test recall</td><td>0.7216</td></tr><tr><td>train accuracy</td><td>0.86853</td></tr><tr><td>train_loss</td><td>0.48441</td></tr><tr><td>train_time</td><td>826.279</td></tr><tr><td>val accuracy</td><td>0.677</td></tr><tr><td>val_loss</td><td>1.34005</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-baseline-bs64-noise0.05</strong> at: <a href='https://wandb.ai/zpd-project/ososn-project/runs/lp58tvjj' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/lp58tvjj</a><br> View project at: <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250612_134459-lp58tvjj\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb_logger = WandbLogger(disable_logging=False)\n",
        "wandb_logger.initialize(\n",
        "    config=hp_baseline,\n",
        "    name=f\"{hp_baseline[\"model_name\"]}-baseline-bs{hp_baseline[\"batch_size\"]}-noise{hp_baseline[\"noise_level\"]}\",\n",
        "    gpu=PICKED_GPU,\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model: {hp_baseline[\"model_name\"]}, Parameters: {model_size / 1e6:.2f}M\")\n",
        "        \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hp_baseline[\"learning_rate\"])\n",
        "\n",
        "    def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "        since = time.time()\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "            epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "            wandb_logger.log_accuracy(epoch_train_acc, \"train\", epoch+1)\n",
        "\n",
        "            # === Walidacja ===\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "            wandb_logger.log_loss(epoch_train_loss, epoch_val_loss, epoch+1)\n",
        "            val_acc, val_f1, val_recall, val_precision = Report.report_results(val_labels, val_preds, tag=\"val\", wandb_logger=wandb_logger, step=epoch+1)\n",
        "\n",
        "            print(f\"Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}, Val loss: {epoch_val_loss:.4f}, Val acc {val_acc:.4f}\")\n",
        "\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), f\"models/best_model_{hp_baseline[\"model_name\"]}.pth\")\n",
        "\n",
        "        total_time = time.time() - since\n",
        "        wandb_logger.log_time(total_time)\n",
        "\n",
        "        print(f\"Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        return model\n",
        "\n",
        "    # === EWALUACJA ===\n",
        "    def evaluate_model(model, dataloader, wandb_logger, device=\"cuda\"):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        duration = time.time() - start\n",
        "\n",
        "        test_acc, test_f1, test_recall, test_precision = Report.report_results(all_labels, all_preds, tag=\"test\", wandb_logger=wandb_logger)\n",
        "        print(f\"Test acc: {test_acc:.4f}, Test f1: {test_f1:.4f}, Test recall: {test_recall:.4f}, Test precision {test_precision:.4f}\")\n",
        "        print(f\"Inference time: {duration:.2f}s\")\n",
        "\n",
        "\n",
        "    base_model = train_model(model, train_loader, optimizer, criterion, hp_baseline[\"num_epochs\"])\n",
        "    evaluate_model(base_model, test_loader, wandb_logger)\n",
        "    torch.save(base_model.state_dict(), f\"models/final_model_{hp_baseline[\"model_name\"]}.pth\")\n",
        "finally:\n",
        "    wandb_logger.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 8 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FLOPS: 1,818,564,096\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "import torch.nn as nn\n",
        "\n",
        "model.load_state_dict(torch.load(\"models/best_model_resnet18_base.pth\", map_location=device))\n",
        "best_base_model = model.to(device)\n",
        "best_base_model.eval()\n",
        "\n",
        "input_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "flops = FlopCountAnalysis(best_base_model, input_tensor)\n",
        "print(f\"FLOPS: {flops.total():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42.71092224121094"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_model_size_mb(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    return size_all_mb\n",
        "\n",
        "get_model_size_mb(best_base_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETAP 5 - Optymalizacja modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "def apply_pruning(model, current_sparsity):\n",
        "    # Przerzedzanie konwolucyjnych wag, procent current_sparsity np. 0.1 (10%)\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=current_sparsity)\n",
        "\n",
        "def remove_pruning(model):\n",
        "    # Usuwa maski, zachowuje sparsity na stałe\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            if prune.is_pruned(module):\n",
        "                prune.remove(module, 'weight')\n",
        "\n",
        "def count_nonzero_params(model):\n",
        "    nonzero = total = 0\n",
        "    for p in model.parameters():\n",
        "        total += p.numel()\n",
        "        nonzero += p.nonzero().size(0)\n",
        "    return nonzero, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\tomek\\Desktop\\projects\\OSiOwSN_projekt\\resnet-optimization\\wandb\\run-20250612_153916-euppx80t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zpd-project/ososn-project/runs/euppx80t' target=\"_blank\">resnet18-pruned-bs64-noise0.05-sparsity0.75</a></strong> to <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zpd-project/ososn-project/runs/euppx80t' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/euppx80t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11.19M\n",
            "\n",
            "Epoch 1/5\n",
            "Applied pruning: sparsity = 0.00\n",
            "Train loss: 1.5324, Train acc: 0.5510, Val loss: 1.2708, Val acc 0.6354\n",
            "Nonzero params: 11186772 / 11186772 (100.00%)\n",
            "Saved new best model.\n",
            "\n",
            "Epoch 2/5\n",
            "Applied pruning: sparsity = 0.32\n",
            "Train loss: 1.0173, Train acc: 0.7201, Val loss: 1.0790, Val acc 0.7124\n",
            "Nonzero params: 11186772 / 11186772 (100.00%)\n",
            "Saved new best model.\n",
            "\n",
            "Epoch 3/5\n",
            "Applied pruning: sparsity = 0.51\n",
            "Train loss: 0.7260, Train acc: 0.8103, Val loss: 1.0635, Val acc 0.7242\n",
            "Nonzero params: 11180988 / 11186772 (99.95%)\n",
            "Saved new best model.\n",
            "\n",
            "Epoch 4/5\n",
            "Applied pruning: sparsity = 0.65\n",
            "Train loss: 0.4609, Train acc: 0.8772, Val loss: 1.2726, Val acc 0.7056\n",
            "Nonzero params: 11180988 / 11186772 (99.95%)\n",
            "\n",
            "Epoch 5/5\n",
            "Applied pruning: sparsity = 0.75\n",
            "Train loss: 0.2591, Train acc: 0.9273, Val loss: 1.4005, Val acc 0.7194\n",
            "Nonzero params: 11180988 / 11186772 (99.95%)\n",
            "Training complete in 14m 51s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 0.7532, Test f1: 0.7500, Test recall: 0.7532, Test precision 0.7590\n",
            "Inference time: 21.26s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>sparsity</td><td>▁▄▆▇█</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>test f1_score</td><td>▁</td></tr><tr><td>test precision</td><td>▁</td></tr><tr><td>test recall</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁▄▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▂▁</td></tr><tr><td>train_time</td><td>▁</td></tr><tr><td>val accuracy</td><td>▁▇█▇█</td></tr><tr><td>val_loss</td><td>▅▁▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>sparsity</td><td>0.75</td></tr><tr><td>test accuracy</td><td>0.7532</td></tr><tr><td>test f1_score</td><td>0.74996</td></tr><tr><td>test precision</td><td>0.75901</td></tr><tr><td>test recall</td><td>0.7532</td></tr><tr><td>train accuracy</td><td>0.92727</td></tr><tr><td>train_loss</td><td>0.25914</td></tr><tr><td>train_time</td><td>891.154</td></tr><tr><td>val accuracy</td><td>0.7194</td></tr><tr><td>val_loss</td><td>1.40046</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-pruned-bs64-noise0.05-sparsity0.75</strong> at: <a href='https://wandb.ai/zpd-project/ososn-project/runs/euppx80t' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/euppx80t</a><br> View project at: <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250612_153916-euppx80t\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "hp_pruning = {\n",
        "    \"num_epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    # \"weight_decay\": 1e-4,\n",
        "    \"batch_size\": 64,\n",
        "    \"model_name\": \"resnet18\",\n",
        "    \"noise_level\": 0.05,\n",
        "    \"max_sparsity\": 0.75,\n",
        "    \"schedule\": \"log\"\n",
        "}\n",
        "wandb_logger = WandbLogger(disable_logging=False)\n",
        "wandb_logger.initialize(\n",
        "    config=hp_pruning,\n",
        "    name=f\"{hp_pruning['model_name']}-pruned-bs{hp_pruning['batch_size']}-noise{hp_pruning['noise_level']}-sparsity{hp_pruning['max_sparsity']}\",\n",
        "    gpu=PICKED_GPU\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model: {hp_pruning['model_name']}, Parameters: {model_size / 1e6:.2f}M\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hp_pruning[\"learning_rate\"])\n",
        "\n",
        "    def train_model_with_pruning(model, train_loader, val_loader, optimizer, criterion, num_epochs, max_sparsity):\n",
        "        since = time.time()\n",
        "        best_acc = 0.0\n",
        "        best_model_wts = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Pruning schedule\n",
        "            # current_sparsity = max_sparsity * (epoch / (num_epochs - 1))\n",
        "            # logarithmic schedule\n",
        "            current_sparsity = max_sparsity * math.log(epoch + 1) / math.log(num_epochs)\n",
        "\n",
        "            wandb_logger.log_sparsity(current_sparsity, epoch+1)\n",
        "            # Remove old masks (if any)\n",
        "            for name, module in model.named_modules():\n",
        "                if isinstance(module, torch.nn.Conv2d):\n",
        "                    try:\n",
        "                        prune.remove(module, 'weight')\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "            # Apply new sparsity\n",
        "            apply_pruning(model, current_sparsity)\n",
        "            print(f\"Applied pruning: sparsity = {current_sparsity:.2f}\")\n",
        "\n",
        "            # === Train ===\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "            epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "            wandb_logger.log_accuracy(epoch_train_acc, \"train\", epoch + 1)\n",
        "\n",
        "            # === Val ===\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "            wandb_logger.log_loss(epoch_train_loss, epoch_val_loss, epoch + 1)\n",
        "\n",
        "            val_acc, val_f1, val_recall, val_precision = Report.report_results(\n",
        "                val_labels, val_preds, tag=\"val\", wandb_logger=wandb_logger, step=epoch + 1\n",
        "            )\n",
        "\n",
        "            print(f\"Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}, Val loss: {epoch_val_loss:.4f}, Val acc {val_acc:.4f}\")\n",
        "\n",
        "            nonzero, total = count_nonzero_params(model)\n",
        "            print(f\"Nonzero params: {nonzero} / {total} ({100 * nonzero / total:.2f}%)\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                remove_pruning(model)\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(best_model_wts, f\"models/best_model_pruned_{hp_pruning['model_name']}.pth\")\n",
        "                print(\"Saved new best model.\")\n",
        "\n",
        "        total_time = time.time() - since\n",
        "        print(f\"Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "        wandb_logger.log_time(total_time)\n",
        "\n",
        "        if best_model_wts:\n",
        "            remove_pruning(model)\n",
        "            model.load_state_dict(best_model_wts)\n",
        "\n",
        "        return model\n",
        "\n",
        "    # === Ewaluacja ===\n",
        "    def evaluate_model(model, dataloader, wandb_logger):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        duration = time.time() - start\n",
        "\n",
        "        test_acc, test_f1, test_recall, test_precision = Report.report_results(\n",
        "            all_labels, all_preds, tag=\"test\", wandb_logger=wandb_logger\n",
        "        )\n",
        "        print(f\"Test acc: {test_acc:.4f}, Test f1: {test_f1:.4f}, Test recall: {test_recall:.4f}, Test precision {test_precision:.4f}\")\n",
        "        print(f\"Inference time: {duration:.2f}s\")\n",
        "\n",
        "    # === Trening + Ewaluacja ===\n",
        "    model = train_model_with_pruning(\n",
        "        model, train_loader, val_loader, optimizer, criterion,\n",
        "        num_epochs=hp_pruning[\"num_epochs\"],\n",
        "        max_sparsity=hp_pruning[\"max_sparsity\"]\n",
        "    )\n",
        "    evaluate_model(model, test_loader, wandb_logger)\n",
        "    torch.save(model.state_dict(), f\"models/final_model_pruned_{hp_pruning['model_name']}.pth\")\n",
        "\n",
        "finally:\n",
        "    wandb_logger.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rozmiar modelu przed pruningiem (gzip): 40611.58 KB\n",
            "Rozmiar modelu po pruningiem (gzip): 23839.02 KB\n",
            "Stopień kompresji: 1.70x\n",
            "Procent kompresji: 41.30%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def save_model_to_file(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def get_gzipped_model_size(original_path):\n",
        "    # Kompresuje .pth do .gz i zwraca rozmiar w bajtach\n",
        "    gzipped_path = original_path + \".gz\"\n",
        "    with open(original_path, 'rb') as f_in:\n",
        "        with gzip.open(gzipped_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    size = os.path.getsize(gzipped_path)\n",
        "    return size\n",
        "\n",
        "base_path=\"models/best_model_resnet18_base.pth\"\n",
        "pruned_path =\"models/best_model_pruned_resnet18.pth\"\n",
        "# save_model_to_file(model, base_path)\n",
        "save_model_to_file(model, pruned_path)\n",
        "\n",
        "# Kompresja\n",
        "base_size = get_gzipped_model_size(base_path)\n",
        "pruned_size = get_gzipped_model_size(pruned_path)\n",
        "\n",
        "print(f\"Rozmiar modelu przed pruningiem (gzip): {base_size / 1024:.2f} KB\")\n",
        "print(f\"Rozmiar modelu po pruningiem (gzip): {pruned_size / 1024:.2f} KB\")\n",
        "print(f\"Stopień kompresji: {base_size / pruned_size:.2f}x\")\n",
        "print(f\"Procent kompresji: {(1 - pruned_size / base_size) * 100:.2f}%\")\n",
        "\n",
        "# linear 0.5\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 32393.43 KB\n",
        "# Stopień kompresji: 1.26x\n",
        "# Procent kompresji: 20.33%\n",
        "\n",
        "\n",
        "# log 0.75\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 17150.66 KB\n",
        "# Stopień kompresji: 2.37x\n",
        "# Procent kompresji: 57.82%\n",
        "\n",
        "# log 0.75 + weight_decay\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 25955.67 KB\n",
        "# Stopień kompresji: 1.57x\n",
        "# Procent kompresji: 36.16%\n",
        "\n",
        "# log 0.9\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 19072.71 KB\n",
        "# Stopień kompresji: 2.13x\n",
        "# Procent kompresji: 53.09%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 8 time(s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FLOPS: 1,818,564,096\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "import torch.nn as nn\n",
        "\n",
        "model.load_state_dict(torch.load(\"models/best_model_pruned_resnet18.pth\", map_location=device))\n",
        "best_base_model_pruned = model.to(device)\n",
        "best_base_model_pruned.eval()\n",
        "\n",
        "input_tensor = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "flops = FlopCountAnalysis(best_base_model_pruned, input_tensor)\n",
        "print(f\"FLOPS: {flops.total():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETAP 6 - Dodakowe optymalizacje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11.19M\n",
            "ResNet18 before quantization: 44.82 MB\n",
            "ResNet18 quantized size: 11.32 MB\n"
          ]
        }
      ],
      "source": [
        "## przykład kwantyzacji\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# === SETUP ===\n",
        "device = torch.device('cpu')  # Quantization works only on CPU\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "model = model.to(device)\n",
        "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model: {hp_pruning['model_name']}, Parameters: {model_size / 1e6:.2f}M\")\n",
        "model.load_state_dict(torch.load(\"models/best_model_pruned_resnet18.pth\", map_location=device))\n",
        "print(f\"ResNet18 before quantization: {get_model_size(model):.2f} MB\")\n",
        "model.eval()\n",
        "\n",
        "# === FUSE RESNET18 ===\n",
        "def fuse_resnet(model):\n",
        "    torch.quantization.fuse_modules(model, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "    for module_name, module in model.named_children():\n",
        "        if isinstance(module, torchvision.models.resnet.BasicBlock):\n",
        "            torch.quantization.fuse_modules(module, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "            torch.quantization.fuse_modules(module, ['conv2', 'bn2'], inplace=True)\n",
        "        elif isinstance(module, nn.Sequential):\n",
        "            for block_name, block in module.named_children():\n",
        "                if isinstance(block, torchvision.models.resnet.BasicBlock):\n",
        "                    torch.quantization.fuse_modules(block, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "                    torch.quantization.fuse_modules(block, ['conv2', 'bn2'], inplace=True)\n",
        "    return model\n",
        "\n",
        "model = fuse_resnet(model)\n",
        "\n",
        "# === QUANTIZATION CONFIG ===\n",
        "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "# === PREPARE ===\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# === CALIBRATE === (use some training data)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# train_set_full = torchvision.datasets.CIFAR100(root='./data', train=True, download=download, transform=transform_train)\n",
        "# test_set_full = torchvision.datasets.CIFAR100(root='./data', train=False, download=download, transform=transform_test)\n",
        "\n",
        "dataset = torchvision.datasets.FakeData(size=64, image_size=(3, 224, 224), num_classes=1000, transform=transform)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, _) in enumerate(loader):\n",
        "        model(inputs)\n",
        "        if i > 5:\n",
        "            break\n",
        "\n",
        "# === CONVERT ===\n",
        "quantized_model = torch.quantization.convert(model, inplace=True)\n",
        "\n",
        "# === MODEL SIZE ===\n",
        "def get_model_size(model, filename='temp.pth'):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    size_mb = os.path.getsize(filename) / 1e6\n",
        "    os.remove(filename)\n",
        "    return size_mb\n",
        "\n",
        "print(f\"ResNet18 quantized size: {get_model_size(quantized_model):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:2044 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cudnn\\Conv.cpp:386 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:504 [backend fallback]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m quantized_model = quantized_model.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_logger\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, wandb_logger)\u001b[39m\n\u001b[32m    135\u001b[39m inputs = inputs.to(device)\n\u001b[32m    136\u001b[39m labels = labels.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m _, preds = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m    139\u001b[39m all_preds.extend(preds.cpu().numpy())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:152\u001b[39m, in \u001b[36mConvReLU2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    148\u001b[39m     _reversed_padding_repeated_twice = _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m.padding)\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m = F.pad(\n\u001b[32m    150\u001b[39m         \u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d_relu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomek\\.conda\\envs\\llm\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mNotImplementedError\u001b[39m: Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:2044 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cudnn\\Conv.cpp:386 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:504 [backend fallback]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n"
          ]
        }
      ],
      "source": [
        "quantized_model = quantized_model.to(device)\n",
        "evaluate_model(quantized_model, loader, wandb_logger)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
