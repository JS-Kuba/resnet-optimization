{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tUCJ0kT1J602"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO & tips:\n",
        "# - hp tuning - wg instrukcji\n",
        "# - zrobić kwantyzacje post training\n",
        "# - pomiary czasu inferencji (testu) na baseline, pruned i quantized\n",
        "# - pomiary wielkości modelu (pliku/wymaganej pamięci)\n",
        "# - ustawiać nazwę swojego GPU (przy każdym wandb init)\n",
        "# - można wyłączać logowanie\n",
        "# - prezentacja, dokumentacja\n",
        "# - uwaga bo pliki modeli się nadpisują"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WandbLogger:\n",
        "    def __init__(self, disable_logging) -> None:\n",
        "        self.disable_logging = disable_logging\n",
        "\n",
        "    def initialize(self, config, name, gpu):\n",
        "        if not self.disable_logging:\n",
        "            config[\"gpu\"] = gpu\n",
        "\n",
        "            wandb.init(\n",
        "                project=\"ososn-project\",\n",
        "                name=name,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "    def log_loss(self, train_loss, val_loss, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=step)\n",
        "\n",
        "    def log_sparsity(self, sparsity, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({\"sparsity\": sparsity}, step=step)\n",
        "\n",
        "    def log_time(self, time):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({\"train_time\": round(time, 3)})\n",
        "\n",
        "    def log_accuracy(self, accuracy, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} accuracy\": accuracy}, step=step)\n",
        "\n",
        "    def log_f1_score(self, f1_score, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} f1_score\": f1_score}, step=step)\n",
        "\n",
        "    def log_recall(self, recall, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} recall\": recall}, step=step)\n",
        "\n",
        "    def log_precision(self, precision, tag, step):\n",
        "        if not self.disable_logging:\n",
        "            wandb.log({f\"{tag} precision\": precision}, step=step)\n",
        "    \n",
        "    def finish(self):\n",
        "        if not self.disable_logging:\n",
        "            wandb.finish()\n",
        "\n",
        "class Report:\n",
        "    @staticmethod\n",
        "    def report_results(y_true, y_pred, tag, wandb_logger, step=None):\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        wandb_logger.log_accuracy(acc, tag, step)\n",
        "\n",
        "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "        recall = recall_score(y_true, y_pred, average='weighted')\n",
        "        precision = precision_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "        if tag == \"test\":\n",
        "            wandb_logger.log_f1_score(f1, tag, step)\n",
        "            wandb_logger.log_recall(recall, tag, step)\n",
        "            wandb_logger.log_precision(precision, tag, step)\n",
        "        \n",
        "        return acc, f1, recall, precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etap 2 - Wybór danych i modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:07<00:00, 23840739.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jakub\\miniconda3\\envs\\zpd\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Jakub\\miniconda3\\envs\\zpd\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11.23M\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 159\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test f1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test precision \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_baseline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     evaluate_model(base_model, test_loader, wandb_logger)\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# torch.save(base_model.state_dict(), f\"models/final_model_{hp_baseline[\"model_name\"]}.pth\")\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "Cell \u001b[1;32mIn[13], line 93\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     91\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 93\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     94\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     96\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "hp_baseline = {\n",
        "    \"num_epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 64,\n",
        "    \"model_name\": \"resnet18\",\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# === TRANSFORMACJE ===\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "if not os.path.exists('./data/cifar-100-python'):\n",
        "    download = True\n",
        "else:\n",
        "    download = False\n",
        "\n",
        "train_set_full = torchvision.datasets.CIFAR100(root='./data', train=True, download=download, transform=transform_train)\n",
        "test_set_full = torchvision.datasets.CIFAR100(root='./data', train=False, download=download, transform=transform_test)\n",
        "\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(0.9 * len(train_set_full))\n",
        "val_len = len(train_set_full) - train_len\n",
        "train_subset, val_subset = random_split(train_set_full, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "\n",
        "train_subset.dataset.transform = transform_train\n",
        "val_subset.dataset.transform = transform_test\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=hp_baseline[\"batch_size\"], shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=hp_baseline[\"batch_size\"], shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set_full, batch_size=hp_baseline[\"batch_size\"], shuffle=False)\n",
        "\n",
        "wandb_logger = WandbLogger(disable_logging=True)\n",
        "wandb_logger.initialize(\n",
        "    config=hp_baseline, \n",
        "    name=f\"{hp_baseline[\"model_name\"]}-baseline-bs{hp_baseline[\"batch_size\"]}\",\n",
        "    gpu=\"NVIDIA GeForce GTX 1060 6GB\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model: {hp_baseline[\"model_name\"]}, Parameters: {model_size / 1e6:.2f}M\")\n",
        "        \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hp_baseline[\"learning_rate\"])\n",
        "\n",
        "    def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "        since = time.time()\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "            epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "            wandb_logger.log_accuracy(epoch_train_acc, \"train\", epoch+1)\n",
        "\n",
        "            # === Walidacja ===\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "            wandb_logger.log_loss(epoch_train_loss, epoch_val_loss, epoch+1)\n",
        "            val_acc, val_f1, val_recall, val_precision = Report.report_results(val_labels, val_preds, tag=\"val\", wandb_logger=wandb_logger, step=epoch+1)\n",
        "\n",
        "            print(f\"Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}, Val loss: {epoch_val_loss:.4f}, Val acc {val_acc:.4f}\")\n",
        "\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                # torch.save(model.state_dict(), f\"models/best_model_{hp_baseline[\"model_name\"]}.pth\")\n",
        "\n",
        "        total_time = time.time() - since\n",
        "        wandb_logger.log_time(total_time)\n",
        "\n",
        "        print(f\"Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        return model\n",
        "\n",
        "    # === EWALUACJA ===\n",
        "    def evaluate_model(model, dataloader, wandb_logger):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        duration = time.time() - start\n",
        "\n",
        "        test_acc, test_f1, test_recall, test_precision = Report.report_results(all_labels, all_preds, tag=\"test\", wandb_logger=wandb_logger)\n",
        "        print(f\"Test acc: {test_acc:.4f}, Test f1: {test_f1:.4f}, Test recall: {test_recall:.4f}, Test precision {test_precision:.4f}\")\n",
        "        print(f\"Inference time: {duration:.2f}s\")\n",
        "\n",
        "\n",
        "    base_model = train_model(model, train_loader, optimizer, criterion, hp_baseline[\"num_epochs\"])\n",
        "    evaluate_model(base_model, test_loader, wandb_logger)\n",
        "    # torch.save(base_model.state_dict(), f\"models/final_model_{hp_baseline[\"model_name\"]}.pth\")\n",
        "finally:\n",
        "    wandb_logger.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "hp_baseline = {\n",
        "        \"num_epochs\": 10,\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"model_name\": \"resnet18\",\n",
        "        \"noise_level\": 0.05\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B8uD0a9eiC39"
      },
      "outputs": [],
      "source": [
        "# === TRANSFORMACJE ===\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# === MAPOWANIE KLAS ===\n",
        "class_mapping = {\n",
        "    0: [4, 30, 55, 72, 95],\n",
        "    1: [1, 32, 67, 73, 91],\n",
        "    2: [54, 62, 70, 82, 92],\n",
        "    3: [9, 10, 16, 28, 61],\n",
        "    4: [0, 51, 53, 57, 83],\n",
        "    5: [22, 39, 40, 86, 87],\n",
        "    6: [5, 20, 25, 84, 94],\n",
        "    7: [6, 7, 14, 18, 24],\n",
        "    8: [3, 42, 43, 88, 97],\n",
        "    9: [12, 17, 37, 68, 76],\n",
        "    10: [23, 33, 49, 60, 71],\n",
        "    11: [15, 19, 21, 31, 38],\n",
        "    12: [34, 63, 64, 66, 75],\n",
        "    13: [26, 45, 77, 79, 99],\n",
        "    14: [2, 11, 35, 46, 98],\n",
        "    15: [27, 29, 44, 78, 93],\n",
        "    16: [36, 50, 65, 74, 80],\n",
        "    17: [47, 52, 56, 59, 96],\n",
        "    18: [8, 13, 48, 58, 90],\n",
        "    19: [41, 69, 81, 85, 89]\n",
        "}\n",
        "\n",
        "custom_class_names = [\n",
        "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables',\n",
        "    'household electrical device', 'household furniture', 'insects', 'large carnivores',\n",
        "    'large man-made outdoor things', 'large natural outdoor scenes', 'large omnivores and herbivores',\n",
        "    'medium-sized mammals', 'non-insect invertebrates', 'people', 'reptiles',\n",
        "    'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
        "]\n",
        "\n",
        "label_remap = {orig: new for new, orig_list in class_mapping.items() for orig in orig_list}\n",
        "NUM_CLASSES = len(class_mapping)\n",
        "\n",
        "# === PRZYGOTOWANIE DANYCH ===\n",
        "if not os.path.exists('./data/cifar-100-python'):\n",
        "    download = True\n",
        "else:\n",
        "    download = False\n",
        "\n",
        "train_set_full = torchvision.datasets.CIFAR100(root='./data', train=True, download=download, transform=transform_train)\n",
        "test_set_full = torchvision.datasets.CIFAR100(root='./data', train=False, download=download, transform=transform_test)\n",
        "\n",
        "\n",
        "def remap_dataset(dataset):\n",
        "    images, labels = [], []\n",
        "    for img, label in zip(dataset.data, dataset.targets):\n",
        "        if label in label_remap:\n",
        "            images.append(img)\n",
        "            labels.append(label_remap[label])\n",
        "    dataset.data = images\n",
        "    dataset.targets = labels\n",
        "    dataset.classes = custom_class_names\n",
        "    return dataset\n",
        "\n",
        "train_set = remap_dataset(train_set_full)\n",
        "test_set = remap_dataset(test_set_full)\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_len = int(0.9 * len(train_set))\n",
        "val_len = len(train_set) - train_len\n",
        "train_subset, val_subset = random_split(train_set, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "# Szumienie etykiet (10%)\n",
        "def add_label_noise(dataset, noise_level=0.1):\n",
        "    if isinstance(dataset, torch.utils.data.Subset):\n",
        "        for i in range(len(dataset)):\n",
        "            if random.random() < noise_level:\n",
        "                true_index = dataset.indices[i]\n",
        "                dataset.dataset.targets[true_index] = random.randint(0, NUM_CLASSES - 1)\n",
        "    else:\n",
        "        for i in range(len(dataset.targets)):\n",
        "            if random.random() < noise_level:\n",
        "                dataset.targets[i] = random.randint(0, NUM_CLASSES - 1)\n",
        "    return dataset\n",
        "\n",
        "train_subset.dataset.transform = transform_train\n",
        "val_subset.dataset.transform = transform_test\n",
        "\n",
        "train_set = add_label_noise(train_set, noise_level=hp_baseline[\"noise_level\"])\n",
        "\n",
        "# Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=hp_baseline[\"batch_size\"], shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=hp_baseline[\"batch_size\"], shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=hp_baseline[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "wandb version 0.20.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Jakub\\Desktop\\PG\\mgr\\sem3\\ososn\\projekt\\wandb\\run-20250610_020839-3fxqr873</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zpd-project/ososn-project/runs/3fxqr873' target=\"_blank\">resnet18-baseline-bs64-wd-noise0.05</a></strong> to <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zpd-project/ososn-project/runs/3fxqr873' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/3fxqr873</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jakub\\miniconda3\\envs\\zpd\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Jakub\\miniconda3\\envs\\zpd\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11.19M\n",
            "Epoch 1/10\n",
            "Train loss: 2.0039, Train acc: 0.4047, Val loss: 1.8609, Val acc 0.4564\n",
            "Epoch 2/10\n",
            "Train loss: 1.5913, Train acc: 0.5346, Val loss: 1.5716, Val acc 0.5316\n",
            "Epoch 3/10\n",
            "Train loss: 1.3705, Train acc: 0.6005, Val loss: 1.5150, Val acc 0.5656\n",
            "Epoch 4/10\n",
            "Train loss: 1.2731, Train acc: 0.6322, Val loss: 1.9334, Val acc 0.4990\n",
            "Epoch 5/10\n",
            "Train loss: 1.0862, Train acc: 0.6898, Val loss: 1.4793, Val acc 0.5818\n",
            "Epoch 6/10\n",
            "Train loss: 0.9213, Train acc: 0.7389, Val loss: 1.5599, Val acc 0.5732\n",
            "Epoch 7/10\n",
            "Train loss: 0.8297, Train acc: 0.7646, Val loss: 1.7781, Val acc 0.5354\n",
            "Epoch 8/10\n",
            "Train loss: 0.7114, Train acc: 0.7973, Val loss: 1.6468, Val acc 0.5816\n",
            "Epoch 9/10\n",
            "Train loss: 0.5642, Train acc: 0.8414, Val loss: 1.7847, Val acc 0.5524\n",
            "Epoch 10/10\n",
            "Train loss: 0.6082, Train acc: 0.8261, Val loss: 1.8811, Val acc 0.5728\n",
            "Training complete in 7m 12s\n",
            "Test acc: 0.6147, Test f1: 0.6096, Test recall: 0.6147, Test precision 0.6172\n",
            "Inference time: 4.10s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test accuracy</td><td>▁</td></tr><tr><td>test f1_score</td><td>▁</td></tr><tr><td>test precision</td><td>▁</td></tr><tr><td>test recall</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▂▂▁▁</td></tr><tr><td>train_time</td><td>▁</td></tr><tr><td>val accuracy</td><td>▁▅▇▃██▅█▆▇</td></tr><tr><td>val_loss</td><td>▇▂▂█▁▂▆▄▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test accuracy</td><td>0.6147</td></tr><tr><td>test f1_score</td><td>0.60965</td></tr><tr><td>test precision</td><td>0.61717</td></tr><tr><td>test recall</td><td>0.6147</td></tr><tr><td>train accuracy</td><td>0.82607</td></tr><tr><td>train_loss</td><td>0.60823</td></tr><tr><td>train_time</td><td>431.54</td></tr><tr><td>val accuracy</td><td>0.5728</td></tr><tr><td>val_loss</td><td>1.8811</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-baseline-bs64-wd-noise0.05</strong> at: <a href='https://wandb.ai/zpd-project/ososn-project/runs/3fxqr873' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/3fxqr873</a><br/> View project at: <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250610_020839-3fxqr873\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb_logger = WandbLogger(disable_logging=False)\n",
        "wandb_logger.initialize(\n",
        "    config=hp_baseline, \n",
        "    name=f\"{hp_baseline[\"model_name\"]}-baseline-bs{hp_baseline[\"batch_size\"]}-wd-noise{hp_baseline[\"noise_level\"]}\",\n",
        "    gpu=\"NVIDIA GeForce GTX 1060 6GB\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model: {hp_baseline[\"model_name\"]}, Parameters: {model_size / 1e6:.2f}M\")\n",
        "        \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hp_baseline[\"learning_rate\"])\n",
        "\n",
        "    def train_model(model, dataloader, optimizer, criterion, num_epochs):\n",
        "        since = time.time()\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "            epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "            wandb_logger.log_accuracy(epoch_train_acc, \"train\", epoch+1)\n",
        "\n",
        "            # === Walidacja ===\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "            wandb_logger.log_loss(epoch_train_loss, epoch_val_loss, epoch+1)\n",
        "            val_acc, val_f1, val_recall, val_precision = Report.report_results(val_labels, val_preds, tag=\"val\", wandb_logger=wandb_logger, step=epoch+1)\n",
        "\n",
        "            print(f\"Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}, Val loss: {epoch_val_loss:.4f}, Val acc {val_acc:.4f}\")\n",
        "\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), f\"models/best_model_{hp_baseline[\"model_name\"]}.pth\")\n",
        "\n",
        "        total_time = time.time() - since\n",
        "        wandb_logger.log_time(total_time)\n",
        "\n",
        "        print(f\"Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        return model\n",
        "\n",
        "    # === EWALUACJA ===\n",
        "    def evaluate_model(model, dataloader, wandb_logger):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        duration = time.time() - start\n",
        "\n",
        "        test_acc, test_f1, test_recall, test_precision = Report.report_results(all_labels, all_preds, tag=\"test\", wandb_logger=wandb_logger)\n",
        "        print(f\"Test acc: {test_acc:.4f}, Test f1: {test_f1:.4f}, Test recall: {test_recall:.4f}, Test precision {test_precision:.4f}\")\n",
        "        print(f\"Inference time: {duration:.2f}s\")\n",
        "\n",
        "\n",
        "    base_model = train_model(model, train_loader, optimizer, criterion, hp_baseline[\"num_epochs\"])\n",
        "    evaluate_model(base_model, test_loader, wandb_logger)\n",
        "    torch.save(base_model.state_dict(), f\"models/final_model_{hp_baseline[\"model_name\"]}.pth\")\n",
        "finally:\n",
        "    wandb_logger.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'base_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     size_all_mb \u001b[38;5;241m=\u001b[39m (param_size \u001b[38;5;241m+\u001b[39m buffer_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m size_all_mb\n\u001b[1;32m---> 11\u001b[0m get_model_size_mb(\u001b[43mbase_model\u001b[49m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'base_model' is not defined"
          ]
        }
      ],
      "source": [
        "def get_model_size_mb(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    return size_all_mb\n",
        "\n",
        "get_model_size_mb(base_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "def apply_pruning(model, current_sparsity):\n",
        "    # Przerzedzanie konwolucyjnych wag, procent current_sparsity np. 0.1 (10%)\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=current_sparsity)\n",
        "\n",
        "def remove_pruning(model):\n",
        "    # Usuwa maski, zachowuje sparsity na stałe\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            if prune.is_pruned(module):\n",
        "                prune.remove(module, 'weight')\n",
        "\n",
        "def count_nonzero_params(model):\n",
        "    nonzero = total = 0\n",
        "    for p in model.parameters():\n",
        "        total += p.numel()\n",
        "        nonzero += p.nonzero().size(0)\n",
        "    return nonzero, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "wandb version 0.20.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Jakub\\Desktop\\PG\\mgr\\sem3\\ososn\\projekt\\wandb\\run-20250610_131347-bpdum64i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zpd-project/ososn-project/runs/bpdum64i' target=\"_blank\">resnet18-pruned-bs64-wd-noise0.05-sparsity0.75</a></strong> to <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zpd-project/ososn-project/runs/bpdum64i' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/bpdum64i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Jakub\\miniconda3\\envs\\zpd\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Jakub\\miniconda3\\envs\\zpd\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: resnet18, Parameters: 11.19M\n",
            "\n",
            "Epoch 1/10\n",
            "Applied pruning: sparsity = 0.00\n",
            "Train loss: 1.9835, Train acc: 0.4143, Val loss: 2.0057, Val acc 0.4450\n",
            "Nonzero params: 11186772 / 11186772 (100.00%)\n",
            "Saved new best model.\n",
            "\n",
            "Epoch 2/10\n",
            "Applied pruning: sparsity = 0.23\n",
            "Train loss: 1.5233, Train acc: 0.5528, Val loss: 2.1859, Val acc 0.4420\n",
            "Nonzero params: 11186772 / 11186772 (100.00%)\n",
            "\n",
            "Epoch 3/10\n",
            "Applied pruning: sparsity = 0.36\n",
            "Train loss: 1.3266, Train acc: 0.6150, Val loss: 1.5564, Val acc 0.5616\n",
            "Nonzero params: 9533967 / 11186772 (85.23%)\n",
            "Saved new best model.\n",
            "\n",
            "Epoch 4/10\n",
            "Applied pruning: sparsity = 0.45\n",
            "Train loss: 1.1006, Train acc: 0.6840, Val loss: 1.4696, Val acc 0.5902\n",
            "Nonzero params: 8613562 / 11186772 (77.00%)\n",
            "Saved new best model.\n",
            "\n",
            "Epoch 5/10\n",
            "Applied pruning: sparsity = 0.52\n",
            "Train loss: 0.9682, Train acc: 0.7236, Val loss: 1.6664, Val acc 0.5606\n",
            "Nonzero params: 7969008 / 11186772 (71.24%)\n",
            "\n",
            "Epoch 6/10\n",
            "Applied pruning: sparsity = 0.58\n",
            "Train loss: 0.7660, Train acc: 0.7842, Val loss: 1.5909, Val acc 0.5886\n",
            "Nonzero params: 7462174 / 11186772 (66.71%)\n",
            "\n",
            "Epoch 7/10\n",
            "Applied pruning: sparsity = 0.63\n",
            "Train loss: 0.6192, Train acc: 0.8261, Val loss: 1.7228, Val acc 0.5784\n",
            "Nonzero params: 7046680 / 11186772 (62.99%)\n",
            "\n",
            "Epoch 8/10\n",
            "Applied pruning: sparsity = 0.68\n",
            "Train loss: 0.5077, Train acc: 0.8592, Val loss: 1.8126, Val acc 0.5888\n",
            "Nonzero params: 6692081 / 11186772 (59.82%)\n",
            "\n",
            "Epoch 9/10\n",
            "Applied pruning: sparsity = 0.72\n",
            "Train loss: 0.4392, Train acc: 0.8753, Val loss: 1.9239, Val acc 0.5800\n",
            "Nonzero params: 6380386 / 11186772 (57.04%)\n",
            "\n",
            "Epoch 10/10\n",
            "Applied pruning: sparsity = 0.75\n",
            "Train loss: 0.4012, Train acc: 0.8832, Val loss: 2.0617, Val acc 0.5780\n",
            "Nonzero params: 6104532 / 11186772 (54.57%)\n",
            "Training complete in 6m 24s\n",
            "Test acc: 0.6259, Test f1: 0.6242, Test recall: 0.6259, Test precision 0.6346\n",
            "Inference time: 3.44s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>sparsity</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>test f1_score</td><td>▁</td></tr><tr><td>test precision</td><td>▁</td></tr><tr><td>test recall</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁▃▄▅▆▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▂▁▁▁</td></tr><tr><td>train_time</td><td>▁</td></tr><tr><td>val accuracy</td><td>▁▁▇█▇█▇██▇</td></tr><tr><td>val_loss</td><td>▆█▂▁▃▂▃▄▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>sparsity</td><td>0.75</td></tr><tr><td>test accuracy</td><td>0.6259</td></tr><tr><td>test f1_score</td><td>0.62417</td></tr><tr><td>test precision</td><td>0.63463</td></tr><tr><td>test recall</td><td>0.6259</td></tr><tr><td>train accuracy</td><td>0.88318</td></tr><tr><td>train_loss</td><td>0.40119</td></tr><tr><td>train_time</td><td>383.903</td></tr><tr><td>val accuracy</td><td>0.578</td></tr><tr><td>val_loss</td><td>2.06171</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-pruned-bs64-wd-noise0.05-sparsity0.75</strong> at: <a href='https://wandb.ai/zpd-project/ososn-project/runs/bpdum64i' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project/runs/bpdum64i</a><br/> View project at: <a href='https://wandb.ai/zpd-project/ososn-project' target=\"_blank\">https://wandb.ai/zpd-project/ososn-project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250610_131347-bpdum64i\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "hp_pruning = {\n",
        "    \"num_epochs\": 10,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    # \"weight_decay\": 1e-4,\n",
        "    \"batch_size\": 64,\n",
        "    \"model_name\": \"resnet18\",\n",
        "    \"noise_level\": 0.05,\n",
        "    \"max_sparsity\": 0.75,\n",
        "    \"schedule\": \"log\"\n",
        "}\n",
        "wandb_logger = WandbLogger(disable_logging=False)\n",
        "wandb_logger.initialize(\n",
        "    config=hp_pruning,\n",
        "    name=f\"{hp_pruning['model_name']}-pruned-bs{hp_pruning['batch_size']}-noise{hp_pruning['noise_level']}-sparsity{hp_pruning['max_sparsity']}\",\n",
        "    gpu=\"NVIDIA GeForce GTX 1060 6GB\"\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model: {hp_pruning['model_name']}, Parameters: {model_size / 1e6:.2f}M\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=hp_pruning[\"learning_rate\"], weight_decay=hp_pruning[\"weight_decay\"])\n",
        "\n",
        "    def train_model_with_pruning(model, train_loader, val_loader, optimizer, criterion, num_epochs, max_sparsity):\n",
        "        since = time.time()\n",
        "        best_acc = 0.0\n",
        "        best_model_wts = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Pruning schedule\n",
        "            # current_sparsity = max_sparsity * (epoch / (num_epochs - 1))\n",
        "            # logarithmic schedule\n",
        "            current_sparsity = max_sparsity * math.log(epoch + 1) / math.log(num_epochs)\n",
        "\n",
        "            wandb_logger.log_sparsity(current_sparsity, epoch+1)\n",
        "            # Remove old masks (if any)\n",
        "            for name, module in model.named_modules():\n",
        "                if isinstance(module, torch.nn.Conv2d):\n",
        "                    try:\n",
        "                        prune.remove(module, 'weight')\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "            # Apply new sparsity\n",
        "            apply_pruning(model, current_sparsity)\n",
        "            print(f\"Applied pruning: sparsity = {current_sparsity:.2f}\")\n",
        "\n",
        "            # === Train ===\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "            epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "            wandb_logger.log_accuracy(epoch_train_acc, \"train\", epoch + 1)\n",
        "\n",
        "            # === Val ===\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_preds = []\n",
        "            val_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    val_loss += loss.item() * inputs.size(0)\n",
        "                    val_preds.extend(preds.cpu().numpy())\n",
        "                    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "            wandb_logger.log_loss(epoch_train_loss, epoch_val_loss, epoch + 1)\n",
        "\n",
        "            val_acc, val_f1, val_recall, val_precision = Report.report_results(\n",
        "                val_labels, val_preds, tag=\"val\", wandb_logger=wandb_logger, step=epoch + 1\n",
        "            )\n",
        "\n",
        "            print(f\"Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}, Val loss: {epoch_val_loss:.4f}, Val acc {val_acc:.4f}\")\n",
        "\n",
        "            nonzero, total = count_nonzero_params(model)\n",
        "            print(f\"Nonzero params: {nonzero} / {total} ({100 * nonzero / total:.2f}%)\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                remove_pruning(model)\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(best_model_wts, f\"models/best_model_pruned_{hp_pruning['model_name']}.pth\")\n",
        "                print(\"Saved new best model.\")\n",
        "\n",
        "        total_time = time.time() - since\n",
        "        print(f\"Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "        wandb_logger.log_time(total_time)\n",
        "\n",
        "        if best_model_wts:\n",
        "            remove_pruning(model)\n",
        "            model.load_state_dict(best_model_wts)\n",
        "\n",
        "        return model\n",
        "\n",
        "    # === Ewaluacja ===\n",
        "    def evaluate_model(model, dataloader, wandb_logger):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        duration = time.time() - start\n",
        "\n",
        "        test_acc, test_f1, test_recall, test_precision = Report.report_results(\n",
        "            all_labels, all_preds, tag=\"test\", wandb_logger=wandb_logger\n",
        "        )\n",
        "        print(f\"Test acc: {test_acc:.4f}, Test f1: {test_f1:.4f}, Test recall: {test_recall:.4f}, Test precision {test_precision:.4f}\")\n",
        "        print(f\"Inference time: {duration:.2f}s\")\n",
        "\n",
        "    # === Trening + Ewaluacja ===\n",
        "    model = train_model_with_pruning(\n",
        "        model, train_loader, val_loader, optimizer, criterion,\n",
        "        num_epochs=hp_pruning[\"num_epochs\"],\n",
        "        max_sparsity=hp_pruning[\"max_sparsity\"]\n",
        "    )\n",
        "    evaluate_model(model, test_loader, wandb_logger)\n",
        "    torch.save(model.state_dict(), f\"models/final_model_pruned_{hp_pruning['model_name']}.pth\")\n",
        "\n",
        "finally:\n",
        "    wandb_logger.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
            "Rozmiar modelu po pruningiem (gzip): 25955.67 KB\n",
            "Stopień kompresji: 1.57x\n",
            "Procent kompresji: 36.16%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def save_model_to_file(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def get_gzipped_model_size(original_path):\n",
        "    # Kompresuje .pth do .gz i zwraca rozmiar w bajtach\n",
        "    gzipped_path = original_path + \".gz\"\n",
        "    with open(original_path, 'rb') as f_in:\n",
        "        with gzip.open(gzipped_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    size = os.path.getsize(gzipped_path)\n",
        "    return size\n",
        "\n",
        "base_path=\"models/final_model_resnet18.pth\"\n",
        "pruned_path =\"models/best_model_pruned_resnet18.pth\"\n",
        "# save_model_to_file(model, base_path)\n",
        "save_model_to_file(model, pruned_path)\n",
        "\n",
        "# Kompresja\n",
        "base_size = get_gzipped_model_size(base_path)\n",
        "pruned_size = get_gzipped_model_size(pruned_path)\n",
        "\n",
        "print(f\"Rozmiar modelu przed pruningiem (gzip): {base_size / 1024:.2f} KB\")\n",
        "print(f\"Rozmiar modelu po pruningiem (gzip): {pruned_size / 1024:.2f} KB\")\n",
        "print(f\"Stopień kompresji: {base_size / pruned_size:.2f}x\")\n",
        "print(f\"Procent kompresji: {(1 - pruned_size / base_size) * 100:.2f}%\")\n",
        "\n",
        "# linear 0.5\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 32393.43 KB\n",
        "# Stopień kompresji: 1.26x\n",
        "# Procent kompresji: 20.33%\n",
        "\n",
        "\n",
        "# log 0.75\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 17150.66 KB\n",
        "# Stopień kompresji: 2.37x\n",
        "# Procent kompresji: 57.82%\n",
        "\n",
        "# log 0.75 + weight_decay\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 25955.67 KB\n",
        "# Stopień kompresji: 1.57x\n",
        "# Procent kompresji: 36.16%\n",
        "\n",
        "# log 0.9\n",
        "# Rozmiar modelu przed pruningiem (gzip): 40660.24 KB\n",
        "# Rozmiar modelu po pruningiem (gzip): 19072.71 KB\n",
        "# Stopień kompresji: 2.13x\n",
        "# Procent kompresji: 53.09%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### przykład kwantyzacji\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import os\n",
        "# import copy\n",
        "\n",
        "# # === SETUP ===\n",
        "# device = torch.device('cpu')  # Quantization works only on CPU\n",
        "# model = torchvision.models.resnet18(pretrained=True)\n",
        "# model.eval()\n",
        "\n",
        "# # === FUSE RESNET18 ===\n",
        "# # Fuse conv, bn, relu layers in basic blocks\n",
        "# def fuse_resnet(model):\n",
        "#     # Fuse initial layers\n",
        "#     torch.quantization.fuse_modules(model, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "\n",
        "#     # Fuse each BasicBlock\n",
        "#     for module_name, module in model.named_children():\n",
        "#         if isinstance(module, torchvision.models.resnet.BasicBlock):\n",
        "#             torch.quantization.fuse_modules(module, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "#             torch.quantization.fuse_modules(module, ['conv2', 'bn2'], inplace=True)\n",
        "#         elif isinstance(module, nn.Sequential):\n",
        "#             for block_name, block in module.named_children():\n",
        "#                 if isinstance(block, torchvision.models.resnet.BasicBlock):\n",
        "#                     torch.quantization.fuse_modules(block, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "#                     torch.quantization.fuse_modules(block, ['conv2', 'bn2'], inplace=True)\n",
        "#     return model\n",
        "\n",
        "# model = fuse_resnet(model)\n",
        "\n",
        "# # === QUANTIZATION CONFIG ===\n",
        "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "# # === PREPARE ===\n",
        "# torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# # === CALIBRATE === (use some training data)\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "# dataset = torchvision.datasets.FakeData(size=64, image_size=(3, 224, 224), num_classes=1000, transform=transform)\n",
        "# loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for i, (inputs, _) in enumerate(loader):\n",
        "#         model(inputs)\n",
        "#         if i > 5:  # few batches are enough\n",
        "#             break\n",
        "\n",
        "# # === CONVERT ===\n",
        "# quantized_model = torch.quantization.convert(model, inplace=True)\n",
        "\n",
        "# # === MODEL SIZE ===\n",
        "# def get_model_size(model, filename='temp.pth'):\n",
        "#     torch.save(model.state_dict(), filename)\n",
        "#     size_mb = os.path.getsize(filename) / 1e6\n",
        "#     os.remove(filename)\n",
        "#     return size_mb\n",
        "\n",
        "# print(f\"ResNet18 quantized size: {get_model_size(quantized_model):.2f} MB\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
